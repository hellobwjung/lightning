{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6016520f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:01:59.689730Z",
     "start_time": "2024-01-26T16:01:57.973619Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91baa7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T15:39:33.353001Z",
     "start_time": "2024-01-26T15:39:33.350384Z"
    }
   },
   "source": [
    "## find the train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96795b7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:01:59.723655Z",
     "start_time": "2024-01-26T16:01:59.694574Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3124b631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:01:59.731051Z",
     "start_time": "2024-01-26T16:01:59.725170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch._C.Generator object at 0x136ddc670>\n"
     ]
    }
   ],
   "source": [
    "train_set_size = int(len(train_set)*0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size],\n",
    "                                        generator=seed)\n",
    "print(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "627572eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:04:54.156109Z",
     "start_time": "2024-01-26T16:04:54.153302Z"
    }
   },
   "outputs": [],
   "source": [
    "a = next(iter(train_set))\n",
    "image, label = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81e47a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:05:56.359501Z",
     "start_time": "2024-01-26T16:05:56.345724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.1098, 0.2627, 0.9490, 0.9961, 1.0000, 0.7725, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.4667, 0.7098, 0.9922, 0.9922, 0.9922, 0.9922, 0.7686, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0392, 0.3137, 0.7765,\n",
       "          0.9686, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9333, 0.3843,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.1020, 0.7098, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7686, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4588, 0.7020, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.3569, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
       "          0.0745, 0.4902, 0.9569, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9412, 0.2314, 0.1804, 0.1804, 0.0078, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.6902, 0.6588, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7686, 0.4039,\n",
       "          0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.5020, 0.8549,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9255, 0.7961, 0.0902, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.1059, 0.0627, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.8667, 0.2510, 0.0431, 0.3725, 0.3725,\n",
       "          0.3725, 0.3725, 0.3725, 0.9490, 0.7137, 0.3725, 0.2902, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9451, 0.2431, 0.3765, 0.6667, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9098, 0.3255,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9216, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9843, 0.8863, 0.9490, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9412,\n",
       "          0.2275, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.2549, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.2549, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9686, 0.8039, 0.3412,\n",
       "          0.0980, 0.2549, 0.7490, 0.9922, 0.9922, 0.9922, 0.9922, 0.9020,\n",
       "          0.2078, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9294, 0.4471, 0.4471,\n",
       "          0.5255, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.5098,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8314, 0.1490,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2039, 0.8863, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.8902, 0.2471, 0.0235, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2706, 0.8863,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.9922, 0.9922, 0.9922, 0.5412, 0.2824, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2275,\n",
       "          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "          0.8784, 0.2549, 0.2549, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28987caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:05:23.665724Z",
     "start_time": "2024-01-26T16:05:23.435753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a5902580>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdfd96e",
   "metadata": {},
   "source": [
    "## define the test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f23215f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:01:59.737983Z",
     "start_time": "2024-01-26T16:01:59.733284Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(28*28, 64),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(64, 3))\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(3, 64), \n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(64, 28*28))\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120e54c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:01:59.745489Z",
     "start_time": "2024-01-26T16:01:59.739356Z"
    }
   },
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y, x = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        val_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        test_loss = F.mse_loss(x_hat, x)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e54db5",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adc76c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:01:59.781306Z",
     "start_time": "2024-01-26T16:01:59.748187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LitAutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (l1): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (l1): Sequential(\n",
      "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=784, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "autoencoder = LitAutoEncoder(Encoder(), Decoder())\n",
    "print(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2181d4",
   "metadata": {},
   "source": [
    "## train with the test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d564b1e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-26T16:02:04.631986Z",
     "start_time": "2024-01-26T16:01:59.783890Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K\n",
      "1 | decoder | Decoder | 51.2 K\n",
      "------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c6c16e37b344b69173064a6cafc4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS device does not support linear for non-float inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# initialize the Trainer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(limit_train_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# test the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model\u001b[38;5;241m=\u001b[39mautoencoder, dataloaders\u001b[38;5;241m=\u001b[39mDataLoader(test_set)) \n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mLitAutoEncoder.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m y, x \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[1;32m     20\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(x_hat, x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ML/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS device does not support linear for non-float inputs"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set)\n",
    "valid_loader = DataLoader(valid_set)\n",
    "\n",
    "\n",
    "# initialize the Trainer\n",
    "trainer = L.Trainer(limit_train_batches=256, max_epochs=10 )\n",
    "\n",
    "trainer.fit(autoencoder, train_loader, valid_loader)\n",
    "\n",
    "# test the model\n",
    "trainer.test(model=autoencoder, dataloaders=DataLoader(test_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b803a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882617b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20890a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4c61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05ece8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
